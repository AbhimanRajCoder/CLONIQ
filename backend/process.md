# Code Plagiarism Detection Backend â€“ Process & Architecture

---

## ðŸ— Project Structure

```
backend/
â”œâ”€â”€ requirements.txt
â””â”€â”€ app/
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ main.py                    # FastAPI entrypoint & app setup
    â”œâ”€â”€ api/
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â””â”€â”€ routes/
    â”‚       â”œâ”€â”€ __init__.py
    â”‚       â””â”€â”€ analyze.py         # FastAPI endpoints & orchestrations
    â”œâ”€â”€ core/
    â”‚   â””â”€â”€ __init__.py
    â”œâ”€â”€ models/
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â””â”€â”€ schemas.py             # Pydantic models & request schemas
    â”œâ”€â”€ services/
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”œâ”€â”€ ast_parser.py          # Source parsing + subtree hashing with line tracking
    â”‚   â”œâ”€â”€ normalizer.py          # AST normalization (canonical renaming, docstring removal)
    â”‚   â”œâ”€â”€ similarity.py          # Jaccard similarity + matching region extraction
    â”‚   â”œâ”€â”€ advanced_similarity.py # CFG and Dataflow similarity analyses
    â”‚   â”œâ”€â”€ llm_judge.py           # ðŸ†• Gemini AI Semantic Judge (triggers at â‰¥ 0.70)
    â”‚   â”œâ”€â”€ metrics.py             # Structural AST metrics (depth, function count, etc.)
    â”‚   â”œâ”€â”€ visualization.py       # AST tree visualization in lightweight JSON
    â”‚   â”œâ”€â”€ analysis_orchestrator.py # Unified analysis pipeline + LLM summary aggregation
    â”‚   â”œâ”€â”€ graph_builder.py       # Similarity graph, matrix, and cluster generation
    â”‚   â””â”€â”€ github_service.py      # GitHub repository fetching logic
    â””â”€â”€ utils/
        â”œâ”€â”€ __init__.py
        â”œâ”€â”€ types.py               # Shared types (SubtreeInfo, FileAnalysis)
        â””â”€â”€ zip_handler.py         # In-memory ZIP extraction
```

---

## ðŸ”„ The Processing Pipeline

```
Upload (.py files or .zip)
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. zip_handler.py      â”‚  â† Extract .py files from ZIP (if applicable)
â”‚     extract_py_files()  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. ast_parser.py       â”‚  â† Convert source code â†’ AST
â”‚     parse_code()        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. normalizer.py       â”‚  â† Rename vars/funcs, replace constants,
â”‚     normalize_ast()     â”‚     strip docstrings, preserve line numbers
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. ast_parser.py            â”‚  â† SHA-256 hash per subtree node,
â”‚     generate_subtree_hashes()â”‚     with start_line / end_line tracking
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  5. Structural Similarity Engine â”‚  â† AST + CFG + DataFlow weighted
â”‚     similarity.py                â”‚     Jaccard similarity
â”‚     advanced_similarity.py       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â–¼
       Score â‰¥ 0.70 ?
        â•±         â•²
      YES          NO
       â”‚            â”‚
       â–¼            â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  6. llm_judge.py â”‚â”‚  â† Gemini AI Semantic Judge
â”‚     evaluate_pairâ”‚â”‚     Classifies: STANDARD_ALGORITHM
â”‚                  â”‚â”‚                 TEMPLATE_OR_BOILERPLATE
â”‚                  â”‚â”‚                 LIKELY_COPY
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
         â–¼          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  7. analysis_orchestrator.py     â”‚  â† Unified response assembly
â”‚     Graph + Matrix + Clusters    â”‚     + LLM summary aggregation
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â–¼
       API JSON Response
```

---

## ï¿½ Step-by-Step Details

### Step 1 â€“ Source Parsing
Uses Python's built-in `ast.parse()` to convert source code into an Abstract Syntax Tree.
- Comments, whitespace, and formatting are automatically discarded.
- `SyntaxError` is caught and reported per file without crashing the pipeline.

### Step 2 â€“ AST Normalization
Applies transformations so structurally identical code produces the same tree regardless of:
| Transformation | Before | After |
|---|---|---|
| Variable names | `result`, `answer` | `var_1`, `var_2` |
| Function names | `add_numbers`, `sum_values` | `func_1`, `func_2` |
| Constants | `42`, `"hello"`, `3.14` | `"CONST"` |
| Docstrings | `"""Adds two numbers."""` | *(removed)* |

**Line numbers (`lineno`, `end_lineno`) are preserved** through normalization for downstream region tracking.

### Step 3 â€“ Subtree Hashing with Line Tracking
Every meaningful AST node gets a fingerprint:
```
hash = SHA-256( NodeType | sorted(child_hashes) )
```

Trivial nodes (bare `Name`, operators like `Add`, `Store`) are hashed but **not tracked** as standalone regions â€” they only contribute to their parent's hash.

Each tracked hash is stored as a `SubtreeInfo`:
```json
{
    "hash": "abc123...",
    "start_line": 5,
    "end_line": 12
}
```

### Step 4 â€“ Structural Similarity (AST + CFG + DataFlow)
For each file pair, three layers are computed:

| Layer | What it Captures | Weight |
|-------|-----------------|--------|
| AST Jaccard | Subtree hash overlap | 40% |
| CFG Jaccard | Control flow graph edges | 30% |
| DataFlow Jaccard | Data dependency graph edges | 30% |

$$Final\_Score = 0.4 \times AST + 0.3 \times CFG + 0.3 \times DataFlow$$

For suspicious pairs (similarity â‰¥ 0.5), the system identifies **which subtree hashes are shared** and maps them back to line ranges in both files:
```json
{
    "file1_lines": [5, 12],
    "file2_lines": [8, 15]
}
```

This enables a frontend to **highlight the exact copied regions**.

### Step 5 â€“ Gemini AI Semantic Judge (Score â‰¥ 0.70)

When the structural similarity score **â‰¥ 0.70**, the system invokes the **Gemini AI LLM** to semantically evaluate the code pair. This dramatically reduces false positives.

**What it does:**
- Sends both source codes + similarity breakdown to Gemini
- Receives a structured JSON classification
- Produces a refined verdict combining structural + semantic analysis

**Classifications:**
| Classification | Meaning |
|---|---|
| `STANDARD_ALGORITHM` | Both implement a common known algorithm (e.g., Sieve, BFS/DFS) |
| `TEMPLATE_OR_BOILERPLATE` | Both follow a common public template or coding pattern |
| `LIKELY_COPY` | Suspicious: uncommon structure, identical creative choices |

**Risk Levels (refined verdict):**
| Risk Level | Trigger |
|---|---|
| `NONE` | Standard algorithm â€” expected similarity |
| `LOW` | Template/boilerplate code |
| `MEDIUM` | LIKELY_COPY with score 0.70â€“0.85 |
| `HIGH` | LIKELY_COPY with score 0.85â€“0.95 |
| `CRITICAL` | LIKELY_COPY with score â‰¥ 0.95 |

**Key design principle:** When uncertain, the prompt instructs Gemini to prefer `STANDARD_ALGORITHM` â€” conservative by design.

---

## ðŸš€ API Endpoints

### `GET /`
Health check. Returns service status and available endpoints.

### `POST /analyze`
Upload multiple `.py` files as multipart form data.
```bash
curl -X POST http://127.0.0.1:8000/analyze \
  -F "files=@student_a.py" \
  -F "files=@student_b.py" \
  -F "files=@student_c.py"
```

### `POST /analyze-pair`
Upload exactly two files for a quick 1-to-1 comparison.
```bash
curl -X POST http://127.0.0.1:8000/analyze-pair \
  -F "file1=@a.py" \
  -F "file2=@b.py"
```

### `POST /compare-zips`
Upload two `.zip` archives (one per user) to compare files cross-ZIP.
```bash
curl -X POST http://127.0.0.1:8000/compare-zips \
  -F "zip1=@user1_submission.zip" \
  -F "zip2=@user2_submission.zip"
```

### `POST /compare-github-repos`
Accept two GitHub repository URLs, fetch all Python files, and perform cross-repo structural similarity comparison.

### `POST /visualize-ast`
Upload a single `.py` file to get its normalized AST as a lightweight JSON tree structure, ideal for frontend D3 visualizations.

### `POST /structure-summary`
Upload a single `.py` file to receive its overall structural metrics: `ast_depth`, `function_count`, `loop_count`, `if_count`, `basic_cyclomatic_complexity`, and total/unique subtree counts.

### `GET /similarity-graph`
Returns a graph structure (nodes and edges) of files parsed in the latest `/analyze` or `/compare-zips` request with similarity score `>= 0.5`.

### `GET /similarity-matrix`
Returns a 2D matrix representing pairwise similarities between all files from the latest analysis, suitable for rendering a heatmap.

### `GET /clusters`
Finds strongly connected components of files (similarity `>= 0.75`) to detect highly suspicious plagiarism rings.

---

## ðŸ“¤ Response Format

```json
{
  "analysis_id": "uuid-v4",
  "summary": {
    "total_files": 4,
    "suspicious_pairs_count": 2,
    "highest_similarity": 0.87,
    "cluster_count": 1
  },
  "similarity": {
    "pairs": [
      {
        "file1": "studentA.py",
        "file2": "studentB.py",
        "similarity_score": 0.87,
        "matching_regions": [
          {
            "file1_lines": [1, 3],
            "file2_lines": [1, 3],
            "file1_code": [{"line_number": 1, "code": "def function():"}],
            "file2_code": [{"line_number": 1, "code": "def function():"}]
          }
        ],
        "llm_verdict": {
          "classification": "LIKELY_COPY",
          "confidence": "HIGH",
          "algorithm_detected": "NONE",
          "reasoning": "Both files share uncommon structural patterns..."
        },
        "refined_verdict": {
          "refined_classification": "LIKELY_COPY",
          "refined_risk_level": "HIGH",
          "original_structural_score": 0.87,
          "llm_confidence": "HIGH",
          "algorithm_detected": "NONE",
          "reasoning": "Both files share uncommon structural patterns...",
          "recommendation": "High structural similarity confirmed by LLM as likely copied. Recommend manual review."
        }
      }
    ]
  },
  "llm_summary": {
    "pairs_evaluated_by_llm": 2,
    "classification_breakdown": {"LIKELY_COPY": 1, "STANDARD_ALGORITHM": 1},
    "risk_level_breakdown": {"HIGH": 1, "NONE": 1},
    "likely_copy_count": 1,
    "standard_algorithm_count": 1,
    "template_count": 0
  },
  "metadata": {
    "analysis_type": "multi_file",
    "timestamp": "2026-03-01T00:00:00Z",
    "llm_enabled": true
  }
}
```

> **Note:** `llm_verdict` and `refined_verdict` fields appear only on pairs with `similarity_score >= 0.70` and when `GEMINI_API_KEY` is configured. Pairs below 0.70 still appear (if above the 0.5 threshold) but without LLM evaluation.

---

## ðŸ›  Setup & Run

```bash
# From the backend directory
cd backend

# Create and activate virtual environment
python3 -m venv venv
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt

# Start the server
uvicorn app.main:app --reload
```

Interactive API docs: **http://127.0.0.1:8000/docs**
